{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_LINKS = 'data/input/links.txt' # Path to the file with the links to the pokemon pages\n",
    "PATH_TMP_LINKS = 'data/tmp/links_left_to_scrape.txt'\n",
    "\n",
    "# Check if the file 'links_left_to_scrape.txt' exists\n",
    "# If not, create it from the file 'links.txt'\n",
    "if not os.path.exists(PATH_TMP_LINKS):\n",
    "    with open(PATH_LINKS, 'r') as f:\n",
    "        for line in f:\n",
    "            with open(PATH_TMP_LINKS, 'a') as f2:\n",
    "                f2.write(line)\n",
    "\n",
    "URLS = []\n",
    "\n",
    "# Read the links from the file 'links_left_to_scrape.txt'\n",
    "with open(PATH_TMP_LINKS, 'r') as f:\n",
    "    for line in f:\n",
    "        URLS.append(line.replace('\\n','').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_japanese_characters(input_string):\n",
    "    cleaned_string = ''.join([i for i in input_string if not re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\",i)])\n",
    "    return cleaned_string\n",
    "\n",
    "def erase_file(filepath, URL):\n",
    "    # Erase the link from the input file\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(filepath, 'w') as f:\n",
    "        for line in lines:\n",
    "            if line.strip('\\n') != URL:\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_webpage(URL):\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(URL)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data from {URL}. Status code: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract data based on the structure of the webpage\n",
    "    webpage_name = URL.split('/')[-1].split('.')[0]\n",
    "    print(f'Getting data from {webpage_name}...')\n",
    "\n",
    "    with open(f'data/tmp/{webpage_name}.txt', 'w') as f:\n",
    "        f.write('')\n",
    "    \n",
    "    e_rows = None\n",
    "    if webpage_name == 'transferonly':\n",
    "        e_rows = soup.select('table:nth-of-type(1) tr td:nth-of-type(2) a')\n",
    "    else:\n",
    "        e_rows = soup.select('table:nth-of-type(2) tr td:nth-of-type(3) a')\n",
    "    \n",
    "    for e_row in e_rows:\n",
    "        name = remove_japanese_characters(e_row.text)\n",
    "\n",
    "        with open(f'data/tmp/{webpage_name}.txt', 'a') as f:\n",
    "            if name != '':\n",
    "                f.write(f'{name}\\n')\n",
    "\n",
    "    erase_file(PATH_TMP_LINKS, URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from paldeapokedex...\n",
      "Getting data from kitakamipokedex...\n",
      "Getting data from transferonly...\n"
     ]
    }
   ],
   "source": [
    "for URL in URLS:\n",
    "    get_data_from_webpage(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the file 'links_left_to_scrape.txt'\n",
    "os.remove(PATH_TMP_LINKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty file to store the Pokemon valid in Regulation E\n",
    "with open('data/output/pokemon_regE.txt', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "# Join the files into one\n",
    "webpages = os.listdir('data/tmp')\n",
    "for webpage in webpages:\n",
    "    with open(f'data/tmp/{webpage}', 'r') as f:\n",
    "        with open('data/output/pokemon_regE.txt', 'a') as f2:\n",
    "            for line in f:\n",
    "                f2.write(line)\n",
    "\n",
    "# Create a dict with keys as Pokemon names and values as the number of times they appear in all the files\n",
    "pokemons = {}\n",
    "with open('data/output/pokemon_regE.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip('\\n') in pokemons.keys():\n",
    "            pokemons[line.strip('\\n')] += 1\n",
    "        else:\n",
    "            pokemons[line.strip('\\n')] = 1\n",
    "\n",
    "with open('data/output/pokemon_regE.txt', 'w') as f:\n",
    "    for pokemon in pokemons:\n",
    "        f.write(f'{pokemon}\\n')\n",
    "\n",
    "# Delete the files with the data from the webpages\n",
    "for webpage in webpages:\n",
    "    os.remove(f'data/tmp/{webpage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
